{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamPrzychodniPrivate/somali-radios-with-ai-for-food-security/blob/main/1_phase/Radio_Ergo_Somali_Speech_to_Text_for_Food_Security.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2O_uvrPy6h2"
      },
      "source": [
        "# 📻 Radio Ergo Somali Speech-to-Text for Food Security\n",
        "\n",
        "## 🎯 Project Overview\n",
        "\n",
        "**Radio Ergo SoundCloud Audio Downloader and Transcriber**\n",
        "\n",
        "This notebook automates the end-to-end process of downloading Somali-language audio from **Radio Ergo’s SoundCloud channel** and transcribing it into readable text. It also includes a comprehensive evaluation of three different **speech-to-text models** to determine the most reliable solution for Somali transcription in support of food security analysis.\n",
        "\n",
        "📘 **Full Project Report:**  \n",
        "  - 📄 [Leveraging Local Radio for Real-Time Food Security Insights: An AI-Powered Approach](https://docs.google.com/document/d/1CvSWsIIZN1jriA02DhczZwuu_-vq87tMO50ZeyycRzY/edit?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 📥 Data Collection – Audio Download System\n",
        "\n",
        "The notebook includes a fully automated system for acquiring audio data from Radio Ergo:\n",
        "\n",
        "### ✅ Features:\n",
        "\n",
        "- **🔗 URL Validation:**  \n",
        "  Ensures SoundCloud links are properly formatted.\n",
        "\n",
        "- **📆 Date-Based Extraction:**  \n",
        "  Extracts track links from Radio Ergo’s profile within a specified date range.\n",
        "\n",
        "- **📡 Automated Download:**  \n",
        "  Downloads all matching audio files in **MP3** format.\n",
        "\n",
        "- **💾 Storage Options:**  \n",
        "  Saves audio locally or to **Google Drive** for persistent access and processing.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🧠 Transcription Model Evaluation\n",
        "\n",
        "The notebook tests **three speech-to-text models** to find the most accurate transcription system for Somali-language audio.\n",
        "\n",
        "### 🧪 Model 1: OpenAI Whisper (Standard)\n",
        "\n",
        "- **🔍 Description:** A general-purpose transcription model by OpenAI.\n",
        "- **✅ Language Detection:** Recognizes Somali.\n",
        "- **❌ Major Issue:** Outputs **Arabic script** instead of the **Somali Latin script**.\n",
        "- **📉 Result:** Unusable transcription with repeated nonsensical Arabic words.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Model 2: Whisper Small Somali (`steja/whisper-small-somali`)\n",
        "\n",
        "- **🔍 Description:** A specialized model from Hugging Face fine-tuned for Somali.\n",
        "- **✅ Improvement:** Correctly uses **Somali Latin script**.\n",
        "- **❌ Major Issue:** Exhibits **hallucinatory repetition**, e.g., *\"iyo iyo iyo\"*, *\"dhul dhul dhul\"*.\n",
        "- **📉 Result:** Partially successful, but unreliable due to extreme repetition.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Model 3: Gemini 2.0 Flash (Google)\n",
        "\n",
        "- **🔍 Description:** A high-performance transcription model from Google.\n",
        "- **✅ Improvement:** Produces **accurate**, **coherent**, and **well-structured** text.\n",
        "- **✅ Script:** Correct **Somali Latin script**.\n",
        "- **✅ Repetition:** Minimal and natural.\n",
        "- **📈 Result:** **Success.** Best-performing model for Somali transcription.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 📊 Transcription Analysis\n",
        "\n",
        "The final section of the notebook performs a detailed evaluation of each model’s output using quantitative metrics.\n",
        "\n",
        "### 🧮 Metrics Analyzed:\n",
        "\n",
        "- **🔠 Word Frequencies:** Frequency of unique and repeated words.\n",
        "- **🔁 Repetition Patterns:** Identification of hallucinated or natural repetitions.\n",
        "- **📏 Line Length Statistics:** Structural consistency of transcription lines.\n",
        "\n",
        "### ✅ Outcome:\n",
        "- **Model 1 & 2:** Failed to deliver reliable transcriptions.\n",
        "- **Model 3 (Gemini 2.0 Flash):** Achieved **high accuracy** and **production-quality output**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Final Recommendation\n",
        "\n",
        "For any future transcription of Somali-language audio — especially in humanitarian or food security contexts — **Gemini 2.0 Flash** is currently the most effective and reliable solution. It significantly outperforms general and specialized Whisper models in both accuracy and formatting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBdZ_pbmK0L"
      },
      "source": [
        "#### 📋 INSTRUCTION\n",
        "\n",
        "## ⚡️ How to Speed Up Speech-to-Text Models:\n",
        "\n",
        "1. 🖱️ Click **Resources** in the menu\n",
        "2. 🔄 Select **Change runtime type**\n",
        "3. 🖥️ Choose a **GPU-enabled runtime** 🚀\n",
        "\n",
        "> ℹ️ **Note**: Following these steps will significantly improve processing speed for speech recognition tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhc4P7Qucgn"
      },
      "source": [
        "# 1. Data Collection - Audio Download System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThVhDqWQ7fi"
      },
      "source": [
        "Code below automates downloading audio from SoundCloud by validating URLs, extracting track links from a profile page within a specified date range, and downloading the audio files using `yt-dlp`.\n",
        "\n",
        "It first scrapes a given SoundCloud profile for track links, filters them based on date patterns in the URLs, and downloads matching audio tracks in MP3 format.\n",
        "\n",
        "Additionally, it includes an option to save downloaded files to Google Drive for storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFTIaPYHPg5E",
        "outputId": "8ea95632-8756-4bf0-8244-dd1d8a32ac63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.6.30-py3-none-any.whl.metadata (174 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/174.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Downloading yt_dlp-2025.6.30-py3-none-any.whl (3.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m2.7/3.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.6.30\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install yt-dlp requests beautifulsoup4\n",
        "\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import yt_dlp\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def validate_soundcloud_url(url):\n",
        "    \"\"\"Validate if the provided URL is a SoundCloud URL.\"\"\"\n",
        "    pattern = r'^https?://(?:www\\.)?soundcloud\\.com/[\\w-]+/[\\w-]+'\n",
        "    return bool(re.match(pattern, url))\n",
        "\n",
        "def download_soundcloud_audio(url, output_dir=\"downloads\"):\n",
        "    \"\"\"Download audio from SoundCloud URL.\"\"\"\n",
        "    if not validate_soundcloud_url(url):\n",
        "        print(f\"Invalid SoundCloud URL: {url}\")\n",
        "        return False\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Configure yt-dlp options\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),\n",
        "        'noplaylist': True,\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'quiet': False,\n",
        "        'verbose': False\n",
        "    }\n",
        "\n",
        "    # Download the audio\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            filename = ydl.prepare_filename(info)\n",
        "            base, _ = os.path.splitext(filename)\n",
        "            mp3_file = f\"{base}.mp3\"\n",
        "            print(f\"Downloaded: {mp3_file}\")\n",
        "            return mp3_file\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def get_soundcloud_urls_by_date_range(profile_url, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Get SoundCloud URLs within a specific date range.\n",
        "\n",
        "    Args:\n",
        "        profile_url: The SoundCloud profile URL (e.g., \"https://soundcloud.com/radio-ergo\")\n",
        "        start_date: Start date (datetime object)\n",
        "        end_date: End date (datetime object)\n",
        "\n",
        "    Returns:\n",
        "        A list of SoundCloud URLs within the specified date range\n",
        "    \"\"\"\n",
        "    if not profile_url.endswith('/'):\n",
        "        profile_url = profile_url + '/'\n",
        "\n",
        "    # Fetch the profile page\n",
        "    try:\n",
        "        response = requests.get(profile_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching profile page: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all track links\n",
        "    track_urls = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith('/') and profile_url.split('/')[3] in href and '/sets/' not in href:\n",
        "            full_url = f\"https://soundcloud.com{href}\"\n",
        "            if validate_soundcloud_url(full_url):\n",
        "                track_urls.append(full_url)\n",
        "\n",
        "    # Filter URLs by date\n",
        "    urls_in_range = []\n",
        "\n",
        "    # Month names mapping (both full and abbreviated)\n",
        "    month_names = {\n",
        "        # Full month names\n",
        "        'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6,\n",
        "        'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12,\n",
        "        # Abbreviated month names\n",
        "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'jun': 6, 'jul': 7,\n",
        "        'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
        "    }\n",
        "\n",
        "    for url in track_urls:\n",
        "        # Try to extract date from URL using regex for both formats\n",
        "        # Format 1: DD-month-YYYY (e.g., 11-march-2025)\n",
        "        # Format 2: DD-mon-YYYY (e.g., 09-mar-2025)\n",
        "        date_match = re.search(r'(\\d{1,2})-([a-z]+)-(\\d{4})', url.split('/')[-1], re.IGNORECASE)\n",
        "\n",
        "        if date_match:\n",
        "            day, month_str, year = date_match.groups()\n",
        "\n",
        "            # Convert month name to number\n",
        "            month = month_names.get(month_str.lower())\n",
        "\n",
        "            if month:\n",
        "                try:\n",
        "                    track_date = datetime(int(year), month, int(day))\n",
        "                    if start_date <= track_date <= end_date:\n",
        "                        urls_in_range.append(url)\n",
        "                        print(f\"Found matching URL: {url}\")\n",
        "                except ValueError:\n",
        "                    # Skip invalid dates\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"Could not parse month: {month_str} in URL: {url}\")\n",
        "        else:\n",
        "            # Print URLs that don't match the pattern for debugging\n",
        "            print(f\"URL does not match date pattern: {url}\")\n",
        "\n",
        "    return urls_in_range\n",
        "\n",
        "def download_by_date_range(profile_url, start_date_str, end_date_str, output_dir=None, save_to_drive=True):\n",
        "    \"\"\"\n",
        "    Download SoundCloud tracks within a date range.\n",
        "\n",
        "    Args:\n",
        "        profile_url: The SoundCloud profile URL\n",
        "        start_date_str: Start date in format 'YYYY-MM-DD'\n",
        "        end_date_str: End date in format 'YYYY-MM-DD'\n",
        "        output_dir: Output directory (default: None, which uses date range as folder name)\n",
        "        save_to_drive: Whether to save files to Google Drive (default: True)\n",
        "    \"\"\"\n",
        "    # Parse dates\n",
        "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "\n",
        "    # Create output directory name based on date range if not provided\n",
        "    if not output_dir:\n",
        "        output_dir = f\"soundcloud_{start_date_str}_to_{end_date_str}\"\n",
        "\n",
        "    print(f\"Searching for tracks from {start_date_str} to {end_date_str}...\")\n",
        "\n",
        "    # Option 1: Try to find using the profile page\n",
        "    urls = get_soundcloud_urls_by_date_range(profile_url, start_date, end_date)\n",
        "\n",
        "    # Option 2: If specific URLs are known to exist but weren't found, add them manually\n",
        "    manual_urls = []\n",
        "\n",
        "    # Check if a specific URL was requested but not found\n",
        "    check_url = \"https://soundcloud.com/radio-ergo/idaacadda-09-mar-2025\"\n",
        "    if check_url not in urls:\n",
        "        current_date = start_date\n",
        "        while current_date <= end_date:\n",
        "            day = current_date.day\n",
        "            month = current_date.strftime('%b').lower()  # Get abbreviated month\n",
        "            year = current_date.year\n",
        "\n",
        "            # Try different date formats\n",
        "            potential_urls = [\n",
        "                f\"https://soundcloud.com/radio-ergo/idaacadda-{day:02d}-{month}-{year}\",\n",
        "                f\"https://soundcloud.com/radio-ergo/idaacadda-{day}-{month}-{year}\"\n",
        "            ]\n",
        "\n",
        "            for url in potential_urls:\n",
        "                try:\n",
        "                    response = requests.head(url)\n",
        "                    if response.status_code == 200:\n",
        "                        manual_urls.append(url)\n",
        "                        print(f\"Manually found URL: {url}\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "    # Combine both lists and remove duplicates\n",
        "    all_urls = list(set(urls + manual_urls))\n",
        "\n",
        "    if not all_urls:\n",
        "        print(\"No tracks found in the specified date range.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found {len(all_urls)} tracks in the specified date range.\")\n",
        "    print(\"URLs found:\")\n",
        "    for url in all_urls:\n",
        "        print(f\"- {url}\")\n",
        "\n",
        "    print(f\"\\nStarting download of {len(all_urls)} SoundCloud audios to: {output_dir}\")\n",
        "    downloaded_files = []\n",
        "    for url in all_urls:\n",
        "        result = download_soundcloud_audio(url, output_dir)\n",
        "        if result:\n",
        "            downloaded_files.append(result)\n",
        "        # Add a small delay to avoid overloading the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total URLs processed: {len(all_urls)}\")\n",
        "    print(f\"Successfully downloaded: {len(downloaded_files)}\")\n",
        "    if downloaded_files:\n",
        "        print(\"\\nDownloaded files:\")\n",
        "        for file in downloaded_files:\n",
        "            print(f\"- {file}\")\n",
        "\n",
        "    # Save to Google Drive\n",
        "    if save_to_drive and downloaded_files:\n",
        "        drive_path = \"/content/drive/MyDrive/\" + output_dir\n",
        "        print(f\"\\nSaving files to Google Drive at: {drive_path}\")\n",
        "\n",
        "        # Create directory in Drive if it doesn't exist\n",
        "        if not os.path.exists(drive_path):\n",
        "            os.makedirs(drive_path)\n",
        "\n",
        "        # Copy files to Drive\n",
        "        os.system(f\"cp -r {output_dir}/* {drive_path}/\")\n",
        "        print(f\"Files successfully saved to Google Drive\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "# You can also directly specify URLs if the automatic search doesn't work\n",
        "def download_specific_urls(urls, output_dir=None, save_to_drive=True):\n",
        "    \"\"\"\n",
        "    Download specific SoundCloud URLs.\n",
        "\n",
        "    Args:\n",
        "        urls: List of SoundCloud URLs to download\n",
        "        output_dir: Output directory (default: None, which uses today's date)\n",
        "        save_to_drive: Whether to save files to Google Drive (default: True)\n",
        "    \"\"\"\n",
        "    if not output_dir:\n",
        "        output_dir = f\"soundcloud_radio_ergo__downloads_{datetime.now().strftime('%Y-%m-%d')}\"\n",
        "\n",
        "    print(f\"Starting download of {len(urls)} specific SoundCloud URLs to: {output_dir}\")\n",
        "    downloaded_files = []\n",
        "    for url in urls:\n",
        "        result = download_soundcloud_audio(url, output_dir)\n",
        "        if result:\n",
        "            downloaded_files.append(result)\n",
        "        # Add a small delay to avoid overloading the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total URLs processed: {len(urls)}\")\n",
        "    print(f\"Successfully downloaded: {len(downloaded_files)}\")\n",
        "    if downloaded_files:\n",
        "        print(\"\\nDownloaded files:\")\n",
        "        for file in downloaded_files:\n",
        "            print(f\"- {file}\")\n",
        "\n",
        "    # Save to Google Drive\n",
        "    if save_to_drive and downloaded_files:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully\")\n",
        "        except:\n",
        "            print(\"Not running in Google Colab or Drive already mounted\")\n",
        "        drive_path = \"/content/drive/MyDrive/\" + output_dir\n",
        "        print(f\"\\nSaving files to Google Drive at: {drive_path}\")\n",
        "\n",
        "        # Create directory in Drive if it doesn't exist\n",
        "        if not os.path.exists(drive_path):\n",
        "            os.makedirs(drive_path)\n",
        "\n",
        "        # Copy files to Drive\n",
        "        os.system(f\"cp -r {output_dir}/* {drive_path}/\")\n",
        "        print(f\"Files successfully saved to Google Drive\")\n",
        "\n",
        "    return downloaded_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iD7bq4EpSpj"
      },
      "source": [
        "#### 📋 INSTRUCTION\n",
        "**Run the cell below to download Radio Ergo auditions from SoundCloud within a specific date range and optionally save them to Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il_GWPK7lGeY",
        "outputId": "cf07aa74-efc5-4c84-cd97-a1c51b8476e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for tracks from 2025-03-15 to 2025-03-16...\n",
            "URL does not match date pattern: https://soundcloud.com/radio-ergo/likes\n",
            "URL does not match date pattern: https://soundcloud.com/radio-ergo/sets\n",
            "URL does not match date pattern: https://soundcloud.com/radio-ergo/tracks\n",
            "URL does not match date pattern: https://soundcloud.com/radio-ergo/comments\n",
            "Manually found URL: https://soundcloud.com/radio-ergo/idaacadda-15-mar-2025\n",
            "Manually found URL: https://soundcloud.com/radio-ergo/idaacadda-15-mar-2025\n",
            "Manually found URL: https://soundcloud.com/radio-ergo/idaacadda-16-mar-2025\n",
            "Manually found URL: https://soundcloud.com/radio-ergo/idaacadda-16-mar-2025\n",
            "Found 2 tracks in the specified date range.\n",
            "URLs found:\n",
            "- https://soundcloud.com/radio-ergo/idaacadda-16-mar-2025\n",
            "- https://soundcloud.com/radio-ergo/idaacadda-15-mar-2025\n",
            "\n",
            "Starting download of 2 SoundCloud audios to: soundcloud_2025-03-15_to_2025-03-16\n",
            "[soundcloud] Extracting URL: https://soundcloud.com/radio-ergo/idaacadda-16-mar-2025\n",
            "[soundcloud] radio-ergo/idaacadda-16-mar-2025: Downloading info JSON\n",
            "[soundcloud] None: Downloading webpage\n",
            "[soundcloud] None: Downloading webpage\n",
            "[soundcloud] radio-ergo/idaacadda-16-mar-2025: Downloading info JSON\n",
            "[soundcloud] 2054370328: Downloading hls_mp3 format info JSON\n",
            "[soundcloud] 2054370328: Downloading http_mp3 format info JSON\n",
            "[soundcloud] 2054370328: Downloading hls_opus format info JSON\n",
            "[info] 2054370328: Downloading 1 format(s): hls_opus_0_0\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 359\n",
            "[download] Destination: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.opus\n",
            "[download] 100% of   27.65MiB in 00:00:53 at 534.25KiB/s                 \n",
            "[ExtractAudio] Destination: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.mp3\n",
            "Deleting original file soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.opus (pass -k to keep)\n",
            "Downloaded: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.mp3\n",
            "[soundcloud] Extracting URL: https://soundcloud.com/radio-ergo/idaacadda-15-mar-2025\n",
            "[soundcloud] radio-ergo/idaacadda-15-mar-2025: Downloading info JSON\n",
            "[soundcloud] 2055043028: Downloading hls_mp3 format info JSON\n",
            "[soundcloud] 2055043028: Downloading http_mp3 format info JSON\n",
            "[soundcloud] 2055043028: Downloading hls_opus format info JSON\n",
            "[info] 2055043028: Downloading 1 format(s): hls_opus_0_0\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 359\n",
            "[download] Destination: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.opus\n",
            "[download] 100% of   27.85MiB in 00:00:12 at 2.23MiB/s                   \n",
            "[ExtractAudio] Destination: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.mp3\n",
            "Deleting original file soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.opus (pass -k to keep)\n",
            "Downloaded: soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.mp3\n",
            "\n",
            "Summary:\n",
            "Total URLs processed: 2\n",
            "Successfully downloaded: 2\n",
            "\n",
            "Downloaded files:\n",
            "- soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.mp3\n",
            "- soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.mp3\n",
            "\n",
            "Saving files to Google Drive at: /content/drive/MyDrive/soundcloud_2025-03-15_to_2025-03-16\n",
            "Files successfully saved to Google Drive\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "profile_url = \"https://soundcloud.com/radio-ergo\"  # SoundCloud profile/channel URL\n",
        "start_date = \"2025-03-15\"  # Format: YYYY-MM-DD\n",
        "end_date = \"2025-03-16\"    # Format: YYYY-MM-DD\n",
        "save_to_drive = True       # Set to False if you don't want to save to Google Drive\n",
        "\n",
        "downloaded_files = download_by_date_range(profile_url, start_date, end_date, save_to_drive=save_to_drive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPW-n6E6bVE"
      },
      "source": [
        "# 2. Transcription with Whisper Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hwxA4vuLIQb"
      },
      "source": [
        "## Regular OpenAI Whisper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z4JTzMjaPq1s"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio_files(input_dir, output_dir=None, language=\"so\", save_to_drive=False,\n",
        "                        audio_formats=None):\n",
        "    \"\"\"\n",
        "    Transcribe audio files using OpenAI's Whisper model.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing audio files\n",
        "        output_dir: Directory to save transcripts (default: input_dir + \"_transcripts\")\n",
        "        language: Language code for transcription (default: \"so\" for Somali)\n",
        "        save_to_drive: Whether to save transcripts to Google Drive (default: False)\n",
        "        audio_formats: List of audio formats to process (default: [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"])\n",
        "\n",
        "    Returns:\n",
        "        List of transcript file paths\n",
        "    \"\"\"\n",
        "    # Mount Google Drive at the beginning if save_to_drive is True\n",
        "    if save_to_drive:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully\")\n",
        "        except ImportError:\n",
        "            print(\"Not running in Google Colab or Drive module not available\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {str(e)}\")\n",
        "            print(\"Continuing without Google Drive. Files will only be saved locally.\")\n",
        "            save_to_drive = False  # Disable save_to_drive if mounting fails\n",
        "\n",
        "    # Install whisper if not already installed\n",
        "    try:\n",
        "        import whisper\n",
        "    except ImportError:\n",
        "        print(\"Installing OpenAI Whisper...\")\n",
        "        !pip install -q openai-whisper\n",
        "        import whisper\n",
        "\n",
        "    # Install ffmpeg if needed (required for handling different audio formats)\n",
        "    try:\n",
        "        import subprocess\n",
        "        result = subprocess.run(['ffmpeg', '-version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if result.returncode != 0:\n",
        "            raise ImportError\n",
        "        print(\"ffmpeg is already installed.\")\n",
        "    except (ImportError, FileNotFoundError):\n",
        "        print(\"Installing ffmpeg (required for audio format conversion)...\")\n",
        "        !apt-get update -qq && apt-get install -qq ffmpeg\n",
        "        print(\"ffmpeg installed successfully.\")\n",
        "\n",
        "    import os\n",
        "    import glob\n",
        "\n",
        "    # Define default audio formats if none provided\n",
        "    if audio_formats is None:\n",
        "        audio_formats = [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"]\n",
        "\n",
        "    # Ensure formats have the dot prefix for glob patterns\n",
        "    formats_pattern = [f\"*.{fmt}\" for fmt in audio_formats]\n",
        "\n",
        "    # Set output directory\n",
        "    if not output_dir:\n",
        "        output_dir = \"whisper_transcripts_\" + input_dir\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Loading Whisper model...\")\n",
        "    # Load the Whisper model (options: tiny, base, small, medium, large)\n",
        "    model = whisper.load_model(\"small\")\n",
        "\n",
        "    # Get all audio files with specified formats in the input directory\n",
        "    audio_files = []\n",
        "    for pattern in formats_pattern:\n",
        "        audio_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"No audio files with formats {audio_formats} found in {input_dir}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files to transcribe.\")\n",
        "    print(f\"Formats to process: {', '.join(audio_formats)}\")\n",
        "\n",
        "    transcript_files = []\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        filename = os.path.basename(audio_file)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        file_format = os.path.splitext(filename)[1][1:].lower()  # Get format without dot\n",
        "        transcript_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
        "\n",
        "        print(f\"\\nTranscribing: {filename} (Format: {file_format})\")\n",
        "\n",
        "        try:\n",
        "            # OpenAI's Whisper can handle various audio formats through ffmpeg\n",
        "            # Transcribe audio with specified language\n",
        "            result = model.transcribe(audio_file, language=language, fp16=False, temperature=0.2)\n",
        "\n",
        "            # Save transcript\n",
        "            with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(result[\"text\"])\n",
        "\n",
        "            print(f\"Transcript saved to: {transcript_file}\")\n",
        "            transcript_files.append(transcript_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {filename}: {str(e)}\")\n",
        "\n",
        "            # If there's an error, try converting to WAV and retry\n",
        "            try:\n",
        "                import subprocess\n",
        "                print(f\"Attempting to convert {file_format} to WAV format and retry...\")\n",
        "\n",
        "                # Create a temporary directory for conversion if it doesn't exist\n",
        "                temp_dir = os.path.join(input_dir, \"_temp_conversion\")\n",
        "                if not os.path.exists(temp_dir):\n",
        "                    os.makedirs(temp_dir)\n",
        "\n",
        "                temp_wav = os.path.join(temp_dir, f\"{base_name}.wav\")\n",
        "\n",
        "                # Convert using ffmpeg\n",
        "                cmd = [\n",
        "                    'ffmpeg', '-y', '-i', audio_file,\n",
        "                    '-ar', '16000', '-ac', '1', '-c:a', 'pcm_s16le',\n",
        "                    temp_wav\n",
        "                ]\n",
        "\n",
        "                process = subprocess.run(\n",
        "                    cmd,\n",
        "                    stdout=subprocess.PIPE,\n",
        "                    stderr=subprocess.PIPE\n",
        "                )\n",
        "\n",
        "                if process.returncode == 0:\n",
        "                    print(f\"Successfully converted to WAV: {temp_wav}\")\n",
        "\n",
        "                    # Try transcription again with the WAV file\n",
        "                    result = model.transcribe(temp_wav, language=language, fp16=False, temperature=0.2)\n",
        "\n",
        "                    # Save transcript\n",
        "                    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(result[\"text\"])\n",
        "\n",
        "                    print(f\"Transcript saved to: {transcript_file} (after conversion)\")\n",
        "                    transcript_files.append(transcript_file)\n",
        "                else:\n",
        "                    print(f\"Conversion failed: {process.stderr.decode()}\")\n",
        "\n",
        "            except Exception as retry_err:\n",
        "                print(f\"Retry after conversion failed: {str(retry_err)}\")\n",
        "\n",
        "    # Clean up temporary conversion directory if it exists\n",
        "    temp_dir = os.path.join(input_dir, \"_temp_conversion\")\n",
        "    if os.path.exists(temp_dir):\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(temp_dir)\n",
        "            print(f\"Removed temporary conversion directory: {temp_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to remove temporary directory: {str(e)}\")\n",
        "\n",
        "    # Save to Google Drive if requested\n",
        "    if save_to_drive and transcript_files:\n",
        "        # Use the same directory name for Google Drive saving\n",
        "        drive_dir_name = os.path.basename(output_dir)\n",
        "        drive_path = \"/content/drive/MyDrive/\" + drive_dir_name\n",
        "        print(f\"\\nSaving transcripts to Google Drive at: {drive_path}\")\n",
        "\n",
        "        # Create directory in Drive if it doesn't exist\n",
        "        if not os.path.exists(drive_path):\n",
        "            os.makedirs(drive_path)\n",
        "\n",
        "        # Copy files to Drive\n",
        "        os.system(f\"cp -r {output_dir}/* {drive_path}/\")\n",
        "        print(f\"Transcripts successfully saved to Google Drive\")\n",
        "\n",
        "    print(f\"\\nTranscription Summary:\")\n",
        "    print(f\"Total files processed: {len(audio_files)}\")\n",
        "    print(f\"Transcripts created: {len(transcript_files)}\")\n",
        "\n",
        "    return transcript_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRh8Gq-zv7Qa"
      },
      "source": [
        "#### 📋 INSTRUCTION\n",
        "**Run the cell below to use Whisper to transcribe all Somali audio files in the specified directory and optionally save the transcripts to Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYKIEnOmv3bf",
        "outputId": "5659b5b2-eb96-4e2f-81f7-8ea1c643c2dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error mounting Google Drive: Mountpoint must not already contain files\n",
            "Continuing without Google Drive. Files will only be saved locally.\n",
            "Installing OpenAI Whisper...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "ffmpeg is already installed.\n",
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:05<00:00, 84.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 audio files to transcribe.\n",
            "Formats to process: wav, mp3, m4a, flac, ogg\n",
            "\n",
            "Transcribing: IDAACADDA 16-MAR-2025.mp3 (Format: mp3)\n",
            "Transcript saved to: whisper_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.txt\n",
            "\n",
            "Transcribing: IDAACADDA 15-MAR-2025.mp3 (Format: mp3)\n",
            "Transcript saved to: whisper_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.txt\n",
            "\n",
            "Transcription Summary:\n",
            "Total files processed: 2\n",
            "Transcripts created: 2\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_directory = \"soundcloud_2025-03-15_to_2025-03-16\"\n",
        "\n",
        "# Transcribe all audio files in the directory\n",
        "transcripts = transcribe_audio_files(\n",
        "    input_dir=input_directory,\n",
        "    language=\"so\",  # Somali language code\n",
        "    save_to_drive=True  # Change to True to save to Google Drive\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5j7NXRB2dpc"
      },
      "source": [
        "### Performance Overview  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2uCMWl85sps",
        "outputId": "a874a93e-95c2-431f-f330-769714ab63e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "import textwrap\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def analyze_somali_transcript(file_path, max_lines=None, sample_size=10):\n",
        "    \"\"\"\n",
        "    Analyze a Somali transcript file with various metrics and display results without any visualizations.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the transcript file\n",
        "    max_lines (int, optional): Maximum number of lines to process (None for all)\n",
        "    sample_size (int, optional): Number of sample lines to display\n",
        "\n",
        "    Returns:\n",
        "    dict: Analysis results\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing file: {os.path.basename(file_path)}\")\n",
        "\n",
        "    # Read the file\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            if max_lines:\n",
        "                lines = [file.readline().strip() for _ in range(max_lines)]\n",
        "                lines = [line for line in lines if line]  # Remove empty lines\n",
        "            else:\n",
        "                lines = [line.strip() for line in file.readlines()]\n",
        "                lines = [line for line in lines if line]  # Remove empty lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Basic statistics\n",
        "    num_lines = len(lines)\n",
        "    total_words = sum(len(line.split()) for line in lines)\n",
        "    avg_words_per_line = total_words / num_lines if num_lines > 0 else 0\n",
        "    avg_word_length = sum(len(word) for line in lines for word in line.split()) / total_words if total_words > 0 else 0\n",
        "\n",
        "    # Word frequency analysis\n",
        "    all_words = ' '.join(lines).lower().split()\n",
        "    word_freq = Counter(all_words)\n",
        "    most_common_words = word_freq.most_common(20)\n",
        "\n",
        "    # Check for repetition patterns\n",
        "    repetition_count = 0\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        for i in range(len(words) - 1):\n",
        "            if words[i] == words[i+1]:\n",
        "                repetition_count += 1\n",
        "\n",
        "    # Detect repeating segments (like \"daadii ergo\" in the example)\n",
        "    text = ' '.join(lines)\n",
        "    repeated_segments = []\n",
        "    for segment_len in range(2, 5):  # Check for segments of 2 to 4 words\n",
        "        words = text.split()\n",
        "        segments = [' '.join(words[i:i+segment_len]) for i in range(len(words)-segment_len+1)]\n",
        "        segment_counts = Counter(segments)\n",
        "        repeated = [(segment, count) for segment, count in segment_counts.items() if count > 3]\n",
        "        repeated_segments.extend(repeated)\n",
        "\n",
        "    # Display sample lines with formatting\n",
        "    print(\"\\n==== Sample Lines ====\")\n",
        "    sample_indices = list(range(min(sample_size, num_lines)))\n",
        "    for i in sample_indices:\n",
        "        wrapped_text = textwrap.fill(lines[i], width=80)\n",
        "        print(f\"Line {i+1}: {wrapped_text}\")\n",
        "\n",
        "    # Display statistics\n",
        "    print(\"\\n==== Basic Statistics ====\")\n",
        "    print(f\"Total lines: {num_lines}\")\n",
        "    print(f\"Total words: {total_words}\")\n",
        "    print(f\"Average words per line: {avg_words_per_line:.2f}\")\n",
        "    print(f\"Average word length: {avg_word_length:.2f}\")\n",
        "    print(f\"Adjacent word repetitions: {repetition_count}\")\n",
        "\n",
        "    # Display most common words\n",
        "    print(\"\\n==== Most Common Words ====\")\n",
        "    for word, count in most_common_words[:10]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    # Display repeated segments\n",
        "    if repeated_segments:\n",
        "        print(\"\\n==== Commonly Repeated Segments ====\")\n",
        "        sorted_segments = sorted(repeated_segments, key=lambda x: x[1], reverse=True)\n",
        "        for segment, count in sorted_segments[:10]:\n",
        "            print(f\"'{segment}' appears {count} times\")\n",
        "\n",
        "    # Detect potential sentence boundaries (basic approach for Somali)\n",
        "    try:\n",
        "        # This is a very basic approach - better sentence detection would require Somali-specific tools\n",
        "        sentences = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            # Split by common Somali sentence-ending punctuation\n",
        "            parts = re.split(r'[.!?]', line)\n",
        "            for i, part in enumerate(parts):\n",
        "                if part.strip():\n",
        "                    current_sentence += \" \" + part.strip()\n",
        "                    if i < len(parts) - 1 or any(line.endswith(p) for p in ['.', '!', '?']):\n",
        "                        sentences.append(current_sentence.strip())\n",
        "                        current_sentence = \"\"\n",
        "\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence.strip())\n",
        "\n",
        "        print(f\"\\nApproximate number of sentences: {len(sentences)}\")\n",
        "        print(\"\\n==== Sample Sentences ====\")\n",
        "        for i, sentence in enumerate(sentences[:5]):\n",
        "            wrapped_text = textwrap.fill(sentence, width=80)\n",
        "            print(f\"Sentence {i+1}: {wrapped_text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting sentences: {e}\")\n",
        "\n",
        "    # Analyze line lengths\n",
        "    line_lengths = [len(line.split()) for line in lines]\n",
        "    min_length = min(line_lengths) if line_lengths else 0\n",
        "    max_length = max(line_lengths) if line_lengths else 0\n",
        "\n",
        "    print(\"\\n==== Line Length Analysis ====\")\n",
        "    print(f\"Shortest line: {min_length} words\")\n",
        "    print(f\"Longest line: {max_length} words\")\n",
        "\n",
        "    # Return analysis results\n",
        "    results = {\n",
        "        'num_lines': num_lines,\n",
        "        'total_words': total_words,\n",
        "        'avg_words_per_line': avg_words_per_line,\n",
        "        'avg_word_length': avg_word_length,\n",
        "        'most_common_words': most_common_words,\n",
        "        'repeated_segments': sorted_segments[:10] if repeated_segments else [],\n",
        "        'line_length_range': (min_length, max_length)\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPD0RCS2GJ7W"
      },
      "source": [
        "#### 📋 INSTRUCTION\n",
        "Run the cell below to visualize the transcript generated by Whisper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku-KnssL5wp2",
        "outputId": "4e1be20d-bc8c-4d09-cdef-769cb57baa2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing file: IDAACADDA 15-MAR-2025.txt\n",
            "\n",
            "==== Sample Lines ====\n",
            "Line 1: موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "\n",
            "==== Basic Statistics ====\n",
            "Total lines: 1\n",
            "Total words: 166\n",
            "Average words per line: 166.00\n",
            "Average word length: 5.00\n",
            "Adjacent word repetitions: 165\n",
            "\n",
            "==== Most Common Words ====\n",
            "موضوع: 166\n",
            "\n",
            "==== Commonly Repeated Segments ====\n",
            "'موضوع موضوع' appears 165 times\n",
            "'موضوع موضوع موضوع' appears 164 times\n",
            "'موضوع موضوع موضوع موضوع' appears 163 times\n",
            "\n",
            "Approximate number of sentences: 1\n",
            "\n",
            "==== Sample Sentences ====\n",
            "Sentence 1: موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع موضوع\n",
            "\n",
            "==== Line Length Analysis ====\n",
            "Shortest line: 166 words\n",
            "Longest line: 166 words\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "file_path = \"whisper_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.txt\"\n",
        "# file_path = \"whisper_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.txt\"\n",
        "\n",
        "results = analyze_somali_transcript(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeEhsecQtkLE"
      },
      "source": [
        "### Problem ❌\n",
        "OpenAI's Whisper model detects Somali (`so`) but **transcribes it in Arabic script** instead of **Somali Latin script**. This makes the output unusable for Somali language.  \n",
        "\n",
        "## Observations  \n",
        "- Produces Arabic text instead of Latin script.  \n",
        "- Users report **repeated phrases** and **incorrect words**.  \n",
        "- Confirmed in multiple discussions:  \n",
        "  - [GitHub Issue #234](https://github.com/openai/whisper/discussions/234)  \n",
        "  - [GitHub Issue #2110](https://github.com/openai/whisper/discussions/2110)  \n",
        "\n",
        "## Possible Causes  \n",
        "- Whisper **lacks training on Somali Latin script**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reFzNDodRCP5"
      },
      "source": [
        "## Whisper Small Somali (Specialized Model)\n",
        "\n",
        "### Why?  \n",
        "OpenAI’s Whisper model **transcribes Somali in Arabic script** instead of **Latin script**, making it unsuitable for Somali. Additionally, it produces **inaccurate and repetitive transcriptions**.\n",
        "\n",
        "### Solution\n",
        "We are switching to **Hugging Face’s Whisper model trained specifically for Somali**:  \n",
        "➡️ **Model:** `steja/whisper-small-somali`  \n",
        "➡️ **Benefit:** Ensures accurate Somali transcription in **Latin script**  \n",
        "\n",
        "### Reference  \n",
        "📄 [Hugging Face Model Docs](https://huggingface.co/steja/whisper-small-somali)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nlftEM6C9pgL"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio_files(input_dir, output_dir=None, language=\"so\", save_to_drive=False,\n",
        "                        audio_formats=None):\n",
        "    \"\"\"\n",
        "    Transcribe audio files using HuggingFace's Whisper model trained for Somali.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing the audio files\n",
        "        output_dir: Directory to save transcripts (default: input_dir + \"_transcripts\")\n",
        "        language: Language code for transcription (default: \"so\" for Somali)\n",
        "        save_to_drive: Whether to save transcripts to Google Drive (default: False)\n",
        "        audio_formats: List of audio formats to process (default: [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"])\n",
        "\n",
        "    Returns:\n",
        "        List of transcript file paths\n",
        "    \"\"\"\n",
        "    # Install required packages if not already installed\n",
        "    try:\n",
        "        import transformers\n",
        "    except ImportError:\n",
        "        print(\"Installing transformers...\")\n",
        "        !pip install -q transformers\n",
        "        import transformers\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "        print(\"Installing PyTorch...\")\n",
        "        !pip install -q torch\n",
        "        import torch\n",
        "\n",
        "    try:\n",
        "        import torchaudio\n",
        "    except ImportError:\n",
        "        print(\"Installing torchaudio...\")\n",
        "        !pip install -q torchaudio\n",
        "        import torchaudio\n",
        "\n",
        "    try:\n",
        "        import soundfile\n",
        "    except ImportError:\n",
        "        print(\"Installing soundfile for additional audio format support...\")\n",
        "        !pip install -q soundfile\n",
        "        import soundfile\n",
        "\n",
        "    import os\n",
        "    import glob\n",
        "    from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "\n",
        "    # Define default audio formats if none provided\n",
        "    if audio_formats is None:\n",
        "        audio_formats = [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"]\n",
        "\n",
        "    # Ensure formats have the dot prefix for glob patterns\n",
        "    formats_pattern = [f\"*.{fmt}\" for fmt in audio_formats]\n",
        "\n",
        "    # Set output directory\n",
        "    if not output_dir:\n",
        "        output_dir = \"whisper_small_somali_transcripts_\" + input_dir\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Loading Hugging Face Whisper model (steja/whisper-small-somali)...\")\n",
        "    # Load the Whisper model from Hugging Face\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model and processor\n",
        "    model_id = \"steja/whisper-small-somali\"\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "    # Move model to appropriate device\n",
        "    model.to(device)\n",
        "\n",
        "    # Create the pipeline for automatic speech recognition\n",
        "    transcriber = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        max_new_tokens=128,\n",
        "        chunk_length_s=30,\n",
        "        batch_size=16,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Get all audio files with specified formats in the input directory\n",
        "    audio_files = []\n",
        "    for pattern in formats_pattern:\n",
        "        audio_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"No audio files with formats {audio_formats} found in {input_dir}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files to transcribe.\")\n",
        "    print(f\"Formats to process: {', '.join(audio_formats)}\")\n",
        "\n",
        "    transcript_files = []\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        filename = os.path.basename(audio_file)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        transcript_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
        "        file_format = os.path.splitext(filename)[1][1:].lower()  # Get format without dot\n",
        "\n",
        "        print(f\"\\nTranscribing: {filename} (Format: {file_format})\")\n",
        "\n",
        "        try:\n",
        "            # Transcribe audio without requesting timestamps\n",
        "            result = transcriber(audio_file)\n",
        "\n",
        "            # Extract the transcript text\n",
        "            if isinstance(result, dict) and \"text\" in result:\n",
        "                transcript_text = result[\"text\"]\n",
        "            else:\n",
        "                # Handle different return formats\n",
        "                transcript_text = result\n",
        "\n",
        "            # Save full transcript\n",
        "            with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(transcript_text)\n",
        "\n",
        "            print(f\"Transcript saved to: {transcript_file}\")\n",
        "            transcript_files.append(transcript_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {filename}: {str(e)}\")\n",
        "\n",
        "            # If there's an error with non-WAV formats, try to convert to WAV first\n",
        "            if file_format != \"wav\":\n",
        "                try:\n",
        "                    print(f\"Attempting to convert {file_format} to WAV format and retry...\")\n",
        "\n",
        "                    # Create a temporary directory for conversion if it doesn't exist\n",
        "                    temp_dir = os.path.join(input_dir, \"_temp_conversion\")\n",
        "                    if not os.path.exists(temp_dir):\n",
        "                        os.makedirs(temp_dir)\n",
        "\n",
        "                    temp_wav = os.path.join(temp_dir, f\"{base_name}.wav\")\n",
        "\n",
        "                    # Load and resave as WAV using torchaudio\n",
        "                    try:\n",
        "                        waveform, sample_rate = torchaudio.load(audio_file)\n",
        "                        torchaudio.save(temp_wav, waveform, sample_rate)\n",
        "                        print(f\"Successfully converted to WAV: {temp_wav}\")\n",
        "\n",
        "                        # Try transcription again with the WAV file\n",
        "                        result = transcriber(temp_wav)\n",
        "\n",
        "                        if isinstance(result, dict) and \"text\" in result:\n",
        "                            transcript_text = result[\"text\"]\n",
        "                        else:\n",
        "                            transcript_text = result\n",
        "\n",
        "                        with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                            f.write(transcript_text)\n",
        "\n",
        "                        print(f\"Transcript saved to: {transcript_file} (after conversion)\")\n",
        "                        transcript_files.append(transcript_file)\n",
        "\n",
        "                    except Exception as conv_err:\n",
        "                        print(f\"Conversion failed: {str(conv_err)}\")\n",
        "\n",
        "                except Exception as retry_err:\n",
        "                    print(f\"Retry failed: {str(retry_err)}\")\n",
        "\n",
        "    # Clean up temporary conversion directory if it exists\n",
        "    temp_dir = os.path.join(input_dir, \"_temp_conversion\")\n",
        "    if os.path.exists(temp_dir):\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(temp_dir)\n",
        "            print(f\"Removed temporary conversion directory: {temp_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to remove temporary directory: {str(e)}\")\n",
        "\n",
        "    # Save to Google Drive if requested\n",
        "    if save_to_drive and transcript_files:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully\")\n",
        "        except:\n",
        "            print(\"Not running in Google Colab or Drive already mounted\")\n",
        "        drive_path = \"/content/drive/MyDrive/\" + os.path.basename(output_dir)\n",
        "        print(f\"\\nSaving transcripts to Google Drive at: {drive_path}\")\n",
        "\n",
        "        # Create directory in Drive if it doesn't exist\n",
        "        if not os.path.exists(drive_path):\n",
        "            os.makedirs(drive_path)\n",
        "\n",
        "        # Copy files to Drive\n",
        "        os.system(f\"cp -r {output_dir}/* {drive_path}/\")\n",
        "        print(f\"Transcripts successfully saved to Google Drive\")\n",
        "\n",
        "    print(f\"\\nTranscription Summary:\")\n",
        "    print(f\"Total files processed: {len(audio_files)}\")\n",
        "    print(f\"Transcripts created: {len(transcript_files)}\")\n",
        "\n",
        "    return transcript_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbyq7R1RwgEI"
      },
      "source": [
        "#### 📋 INSTRUCTION   \n",
        "**Run the cell below to use Whisper Small Somali to transcribe all Somali audio files in the specified directory and optionally save the transcripts to Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cw_7S1JCwFXH"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "input_directory = \"soundcloud_2025-03-15_to_2025-03-16\"\n",
        "\n",
        "# Transcribe all audio files in the directory\n",
        "transcripts = transcribe_audio_files(\n",
        "    input_dir=input_directory,\n",
        "    language=\"so\",  # Somali language code\n",
        "    save_to_drive=True  # Change to True to save to Google Drive\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OROcSAzxwZYi"
      },
      "source": [
        "## Evaluation of Whisper Small Somali (Specialized Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQFPl_KYxP9m"
      },
      "source": [
        "### Transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl7DwLr8G30C"
      },
      "source": [
        "#### 📋 INSTRUCTION\n",
        "Run the cell below to visualize the transcript generated this time by Whisper Small Somali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrhvhvKc6Kxn",
        "outputId": "9868c0ab-3eb6-40c2-d99e-eb327fca401c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing file: IDAACADDA 15-MAR-2025.txt\n",
            "Error: File not found at /content/whisper_small_somali_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.txt\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "file_path = \"/content/whisper_small_somali_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.txt\"\n",
        "# file_path = \"/content/whisper_small_somali_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.txt\"\n",
        "\n",
        "results = analyze_somali_transcript(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Flash 2.0 – Somali Transcription Update\n",
        "\n",
        "### Why?  \n",
        "After achieving 45.9% WER with the top-performing Scribe v1 model on the FLEURS benchmark, Gemini Flash 2.0 ranks as the second-best with a 54.4% Word Error Rate (WER).\n",
        "🔗 [See Benchmark Results](https://elevenlabs.io/speech-to-text/somali)\n",
        "\n",
        "### Solution\n",
        "We're now deploying Gemini's in-house Somali transcription model, designed specifically for high accuracy and Latin script output:\n",
        "➡️ **Model:**  Gemini Flash 2.0 Somali\n",
        "➡️ **Benefit:** Improved Somali transcription quality using Gemini-tuned architecture, optimized for real-world use cases.\n",
        "\n",
        "### Reference  \n",
        "📄 [Gemini Flash 2.0 documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)"
      ],
      "metadata": {
        "id": "aSlRYBFQUKhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from typing import Optional, List, Dict\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from datetime import datetime\n",
        "import json\n",
        "import time\n",
        "import subprocess # For ffmpeg\n",
        "import shutil     # For directory cleanup\n",
        "\n",
        "def transcribe_audio_files(\n",
        "    input_dir: str,\n",
        "    api_key: str, # API key is now passed as an argument\n",
        "    output_dir: Optional[str] = None,\n",
        "    language: str = \"so\", # Language hint for Gemini, not a strict filter\n",
        "    save_to_drive: bool = False,\n",
        "    audio_formats: Optional[List[str]] = None,\n",
        "    model: str = \"gemini-2.0-flash\",\n",
        "    retry_count: int = 3,\n",
        "    delay_between_failures: int = 10\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Transcribe audio files using Google Gemini Flash 2.0.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Directory containing audio files.\n",
        "        api_key (str): Your Google Gemini API key.\n",
        "        output_dir (str, optional): Directory to save transcripts.\n",
        "                                    Defaults to input_dir + \"_gemini_transcripts\".\n",
        "        language (str, optional): Language code for transcription (e.g., \"so\" for Somali).\n",
        "                                  This acts as a hint for the model. Defaults to \"so\".\n",
        "        save_to_drive (bool, optional): Whether to save transcripts to Google Drive.\n",
        "                                        Defaults to False.\n",
        "        audio_formats (list, optional): List of audio formats to process.\n",
        "                                        Defaults to [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"].\n",
        "        model (str, optional): The Gemini model to use for transcription.\n",
        "                               Defaults to \"gemini-2.0-flash\".\n",
        "        retry_count (int, optional): Number of times to retry failed transcriptions.\n",
        "                                     Defaults to 3.\n",
        "        delay_between_failures (int, optional): Seconds to wait between retry attempts.\n",
        "                                                Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of transcript file paths created.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"API key cannot be empty. Please provide your Gemini API key.\")\n",
        "\n",
        "    # Mount Google Drive at the beginning if save_to_drive is True\n",
        "    if save_to_drive:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully\")\n",
        "        except ImportError:\n",
        "            print(\"Not running in Google Colab or Drive module not available\")\n",
        "            save_to_drive = False # Disable if not in Colab\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {str(e)}\")\n",
        "            print(\"Continuing without Google Drive. Files will only be saved locally.\")\n",
        "            save_to_drive = False  # Disable save_to_drive if mounting fails\n",
        "\n",
        "    # Check and install ffmpeg if needed (required for handling different audio formats)\n",
        "    try:\n",
        "        subprocess.run(['ffmpeg', '-version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
        "        print(\"ffmpeg is already installed.\")\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "        print(\"Installing ffmpeg (required for audio format conversion)...\")\n",
        "        # For Colab/Debian-based systems\n",
        "        subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
        "        subprocess.run(['apt-get', 'install', '-qq', 'ffmpeg'], check=True)\n",
        "        print(\"ffmpeg installed successfully.\")\n",
        "\n",
        "    # Initialize Gemini client\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    # Define default audio formats if none provided\n",
        "    if audio_formats is None:\n",
        "        audio_formats = [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\"]\n",
        "\n",
        "    # Ensure formats have the dot prefix for glob patterns\n",
        "    formats_pattern = [f\"*.{fmt}\" for fmt in audio_formats]\n",
        "\n",
        "    # Set output directory\n",
        "    if not output_dir:\n",
        "        output_dir = \"gemini_transcripts_\" + os.path.basename(input_dir.rstrip('/'))\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Using Gemini model: {model}\")\n",
        "\n",
        "    # Get all audio files with specified formats in the input directory\n",
        "    audio_files = []\n",
        "    for pattern in formats_pattern:\n",
        "        audio_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"No audio files with formats {audio_formats} found in {input_dir}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files to transcribe.\")\n",
        "    print(f\"Formats to process: {', '.join(audio_formats)}\")\n",
        "\n",
        "    transcript_files = []\n",
        "    temp_conversion_dir = os.path.join(input_dir, \"_temp_gemini_conversion\")\n",
        "    os.makedirs(temp_conversion_dir, exist_ok=True) # Create temp dir once\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        filename = os.path.basename(audio_file)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        file_extension = os.path.splitext(filename)[1][1:].lower() # Get format without dot\n",
        "        transcript_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
        "\n",
        "        # Skip if already transcribed\n",
        "        if os.path.exists(transcript_file):\n",
        "            print(f\"Skipping {filename} - already transcribed\")\n",
        "            transcript_files.append(transcript_file) # Add to list even if skipped\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTranscribing: {filename} (Format: {file_extension})\")\n",
        "\n",
        "        current_audio_path = audio_file\n",
        "        needs_conversion = False\n",
        "\n",
        "        if file_extension != \"mp3\":\n",
        "            needs_conversion = True\n",
        "            temp_mp3_path = os.path.join(temp_conversion_dir, f\"{base_name}.mp3\")\n",
        "            print(f\"Converting {file_extension} to MP3: {audio_file} -> {temp_mp3_path}\")\n",
        "            try:\n",
        "                cmd = [\n",
        "                    'ffmpeg', '-y', '-i', audio_file,\n",
        "                    '-vn', # no video\n",
        "                    '-acodec', 'libmp3lame', # encode to mp3\n",
        "                    '-ar', '16000', # 16 kHz sample rate (common for speech)\n",
        "                    '-ac', '1', # mono audio\n",
        "                    '-b:a', '32k', # bitrate, adjust as needed for quality vs size\n",
        "                    temp_mp3_path\n",
        "                ]\n",
        "                process = subprocess.run(\n",
        "                    cmd,\n",
        "                    stdout=subprocess.PIPE,\n",
        "                    stderr=subprocess.PIPE,\n",
        "                    check=True # Raise CalledProcessError if command fails\n",
        "                )\n",
        "                current_audio_path = temp_mp3_path\n",
        "                print(f\"Successfully converted to MP3.\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"FFmpeg conversion failed for {filename}: {e.stderr.decode()}\")\n",
        "                print(f\"Skipping {filename} due to conversion error.\")\n",
        "                # Log failure\n",
        "                with open(os.path.join(output_dir, \"failed_transcriptions.json\"), \"a\") as f:\n",
        "                    failure_info = {\n",
        "                        \"filename\": filename,\n",
        "                        \"path\": audio_file,\n",
        "                        \"timestamp\": datetime.now().isoformat(),\n",
        "                        \"error\": f\"FFmpeg conversion failed: {e.stderr.decode()}\"\n",
        "                    }\n",
        "                    f.write(json.dumps(failure_info) + \"\\n\")\n",
        "                continue # Skip to next audio file\n",
        "\n",
        "        # Try transcription with retries\n",
        "        for attempt in range(retry_count):\n",
        "            try:\n",
        "                # Read audio file bytes (from original or converted MP3)\n",
        "                with open(current_audio_path, \"rb\") as f:\n",
        "                    audio_bytes = f.read()\n",
        "\n",
        "                # Create a Part with correct MIME type\n",
        "                audio_part = types.Part.from_bytes(\n",
        "                    data=audio_bytes,\n",
        "                    mime_type=\"audio/mp3\"\n",
        "                )\n",
        "\n",
        "                # Create prompt - include language hint\n",
        "                prompt = f\"Generate a transcript of the speech in {language} language.\"\n",
        "\n",
        "                # Request transcript\n",
        "                response = client.models.generate_content(\n",
        "                    model=model,\n",
        "                    contents=[prompt, audio_part]\n",
        "                )\n",
        "\n",
        "                # Return the transcript\n",
        "                if hasattr(response, 'text'):\n",
        "                    transcript_text = response.text\n",
        "                else:\n",
        "                    raise ValueError(\"No transcript text found in the response.\")\n",
        "\n",
        "                # Save the transcript\n",
        "                with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(transcript_text)\n",
        "\n",
        "                print(f\"Successfully transcribed {filename}\")\n",
        "                transcript_files.append(transcript_file)\n",
        "                break # Exit retry loop on success\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt+1}/{retry_count} failed for {filename}: {str(e)}\")\n",
        "                if attempt < retry_count - 1:\n",
        "                    print(f\"Waiting {delay_between_failures} seconds before retrying...\")\n",
        "                    time.sleep(delay_between_failures)\n",
        "                else:\n",
        "                    print(f\"All attempts failed for {filename}\")\n",
        "                    # Log failure\n",
        "                    with open(os.path.join(output_dir, \"failed_transcriptions.json\"), \"a\") as f:\n",
        "                        failure_info = {\n",
        "                            \"filename\": filename,\n",
        "                            \"path\": audio_file,\n",
        "                            \"timestamp\": datetime.now().isoformat(),\n",
        "                            \"error\": str(e)\n",
        "                        }\n",
        "                        f.write(json.dumps(failure_info) + \"\\n\")\n",
        "\n",
        "    # Clean up temporary conversion directory\n",
        "    if os.path.exists(temp_conversion_dir):\n",
        "        try:\n",
        "            shutil.rmtree(temp_conversion_dir)\n",
        "            print(f\"Removed temporary conversion directory: {temp_conversion_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to remove temporary directory: {str(e)}\")\n",
        "\n",
        "    # Save to Google Drive if requested\n",
        "    if save_to_drive and transcript_files:\n",
        "        # Use the same directory name for Google Drive saving\n",
        "        drive_dir_name = os.path.basename(output_dir.rstrip('/'))\n",
        "        drive_path = os.path.join(\"/content/drive/MyDrive/\", drive_dir_name)\n",
        "        print(f\"\\nSaving transcripts to Google Drive at: {drive_path}\")\n",
        "\n",
        "        # Create directory in Drive if it doesn't exist\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "        # Copy files to Drive\n",
        "        copied_count = 0\n",
        "        for f_path in transcript_files:\n",
        "            try:\n",
        "                shutil.copy(f_path, os.path.join(drive_path, os.path.basename(f_path)))\n",
        "                copied_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to copy {f_path} to Drive: {e}\")\n",
        "        print(f\"Successfully copied {copied_count} transcripts to Google Drive.\")\n",
        "\n",
        "    print(f\"\\nTranscription Summary:\")\n",
        "    print(f\"Total files processed: {len(audio_files)}\")\n",
        "    print(f\"Transcripts created: {len(transcript_files)}\")\n",
        "\n",
        "    return transcript_files\n"
      ],
      "metadata": {
        "id": "XaJ0SxmSU7pu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user for the API key manually\n",
        "# gemini_api_key = input(\"Please enter your Gemini API Key: \")"
      ],
      "metadata": {
        "id": "JN2WxAtI5ALh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_directory = \"soundcloud_2025-03-15_to_2025-03-16\"\n",
        "\n",
        "# Transcribe all audio files in the directory\n",
        "transcripts = transcribe_audio_files(\n",
        "    input_dir=input_directory,\n",
        "    api_key=gemini_api_key, # Pass the manually entered API key\n",
        "    language=\"so\", # Somali language code\n",
        "    save_to_drive=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqTPZ5CAa9WB",
        "outputId": "ca9a4e6a-8d16-489b-f472-c4d0c5077a87"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error mounting Google Drive: Mountpoint must not already contain files\n",
            "Continuing without Google Drive. Files will only be saved locally.\n",
            "ffmpeg is already installed.\n",
            "Using Gemini model: gemini-2.0-flash\n",
            "Found 2 audio files to transcribe.\n",
            "Formats to process: wav, mp3, m4a, flac, ogg\n",
            "Skipping IDAACADDA 16-MAR-2025.mp3 - already transcribed\n",
            "\n",
            "Transcribing: IDAACADDA 15-MAR-2025.mp3 (Format: mp3)\n",
            "Successfully transcribed IDAACADDA 15-MAR-2025.mp3\n",
            "Removed temporary conversion directory: soundcloud_2025-03-15_to_2025-03-16/_temp_gemini_conversion\n",
            "\n",
            "Transcription Summary:\n",
            "Total files processed: 2\n",
            "Transcripts created: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "file_path = \"/content/gemini_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 15-MAR-2025.txt\"\n",
        "# file_path = \"/content/gemini_transcripts_soundcloud_2025-03-15_to_2025-03-16/IDAACADDA 16-MAR-2025.txt\"\n",
        "\n",
        "results = analyze_somali_transcript(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkTOaaTtcprA",
        "outputId": "24cf86c0-33ed-47d6-f3e3-3e87b6aa40e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing file: IDAACADDA 15-MAR-2025.txt\n",
            "\n",
            "==== Sample Lines ====\n",
            "Line 1: Halkanina waa Radiyo Ergo ee codka arimaha bini'aadanimada oo fadhigiisu yahay\n",
            "magalada Nairobi ee dalka kenya waxad naga dhageysanaysaan mowjado gaaban ee\n",
            "dherer keedu yahay 22 ka mitir baan una dhiganta 25,670.00 maga hertz. Saacada\n",
            "geeska afrika dhabari marka ay tahay sedexda ilaa afarta gelabnimo\n",
            "Line 2: Waxaad sidoo kale naga dhagaysanaysaan qaara ka tirsan idaacadaha dalka iyo\n",
            "barta aanu ku leenahay internetka ee fadhigeedu yahay radiyo ergo dot o w r g.\n",
            "Line 3: Kulanti wanaag san eedageystayal waa sabti ay bisha maarso sanadku waa 2022 ku\n",
            "soo dhawaada idaca Ergo ee maanta anigu ahsan madaale ayaa la socodsiinayaa\n",
            "qodobada aadka macquul ka doontan waxa ka mid ah xooldhaqatada noolol xumagaa ku\n",
            "waajahay iyo diganka cali waal ee gobal ku nado o markii daqalaah aan iyo biyo\n",
            "yari ay u dhinten in ka badan xoolahoodi. Qoraal sokono wax magalada fasah dherr\n",
            "e gobal ka baydh waa ugu dilah maareynta dhaarasho quusaska kooda markii ay\n",
            "dooreen kooxda al shabaab. Burnaamij ka daqasho otudobaadka naku soo qaada\n",
            "doondo. su’ala ha noo soo waydiiyan xoolaha xaaladaha ay ku xalusiyeen iyo joob\n",
            "calintaa halaqdaarka\n",
            "Line 4: qurarka qimahah qimahaa xoolaha burhanishka gubad iyo cudoodka dadaka fayimaa o\n",
            "dubaai akawargadiin ayahoo xaalada deeganadooodi qudubada aaso o faafaas oo\n",
            "godad aad markalah doontaanaa hadisheeshigii maxaa korinaayaa qurka koroban\n",
            "xamdi diwaay ayaa soo jeeday hesa\n",
            "Line 5: in kabadan 26 milyan oo caruura o kunool dalka suudaan aya u baahan gargara\n",
            "daadaaga halka kudhawaat 13 milyan oo caruuraa ay ka maqan yihiin iskoolada\n",
            "mudada labada sanadood gapdaha ayaa lasheegay kuwa u u badan ee la kulama\n",
            "katarah ayad ka u daran ee ay kamidtahay tadhiyada galmada  taharibintaa iyo\n",
            "guurka qasabka sidao kale dalak suudaan inka badan 12 milyan oo qofah ay heli\n",
            "sugu jirin haatu guudigga daadaagaan cunsigaa sida uu shaegtay hayda qaramo\n",
            "midobah u qabilsan caruurtii yunsif sida guud alamkaa yunsif aya shaegtay in 3,2\n",
            "milyan oo caruuro o kuyaala dhashen tashin sano\n",
            "Line 6: la filayey in aiso face artana faqada dharab ahan sanadkan waxeeda aiso ku\n",
            "dartey in haatid aiso la helin biyo nadiifa faayidha oriya daryeel acafimaad\n",
            "nolosha carurta yaan hala iskarikarto meel aha ay u barusaameeyeen adalagadyah\n",
            "asaa si gaar ah haayeesha ka baxiin dhanka kale inka badan 441,000 caruura ay\n",
            "kunolol dalaka jamhuriyadah dimuqaraadiyay kongo ayayla iskarireen ay bilaayyeen\n",
            "january iyo febrery ay sanadkan iyado qiarkood la dadoday inta badan 14 ilaa 15\n",
            "sana jirta arurtan ayaa lasheegay in qiarkood laka soo qaaday iskoolala iyo\n",
            "wadadiin halka kuwoakala so qaaday deeeganadee ey kunolaayeen waxa laga yaacay\n",
            "kaymaha si tarbar logu fidsan sidaa eashaeqtay hayada dababaadaha carurta\n",
            "eeseydhajildara warqiadadka aiso coteen radhioergo waa nagah intaa murkana\n",
            "qimahayaha kale edhahiya cismaan mudaaliin\n",
            "Line 7: aiskoo sanokoduna san dagaastaayal markale hadio inu kulabaan ogado o wax\n",
            "uguraya eedaadene maanta waxa marba maka kadambeeyisid sidaan arinta ayalahay\n",
            "soo rabaayaa dalalka ka jirtaa oo noolal xumah boodhey dad badan oo soomalida ay\n",
            "ka diimaaki bileen kabiaashidaqalada aya ku tirsanaiyeen dadak aiso wayin u\n",
            "taabateen arintan ayaa aiso wabadin xoolaha daqhataa iyo bereeleeda melaha ay\n",
            "xaaradaa ougu daren kajirtu waxaa kamidah gobalada dhexhe oo dalaka oo 3 hilee\n",
            "roobaad u harriraa ay san kaa dheeyin rooba fiicaan aiso u sosaariin daaqo aiso\n",
            "kunolaayeen karaaan xoolhaoo in kataa badan dadaka ay ku tirsanaaiyeen waxaa\n",
            "kamidah diganka alibaal ee gobal kumidu oo wajahaaya nolal xumah sareysay iyo\n",
            "dalak soomalida markay iskuradhaayeen biyo yari iyo dagaalaan gali afata in kata\n",
            "badanaa a xoolaha ay ku tirsanaiyeen waxoo war baxinta aiso kashaadeegsay o\n",
            "xaalado dadaka asagu qaasaysaa\n",
            "Line 8: warika radhiyaw ayago gooba magaladah oogiyo seinab cabdhulaahi xussein kun iyo\n",
            "afaraan boqol qoysa o kunooldeegaan ka aliiwaal ee gobal ka madux ayaa\n",
            "kuwajahaaya 6 dhib aalo ay u dhibadeen xaalada nolal xumah arinkasa ayakag daran\n",
            "ta markay isaga daaq xumo iyo biyo yaraao udhinta in kata badan xoolaha ay ku\n",
            "tirsanaayeen wax kamidah oyaqa ay qasaysaa cabdhulaahi muxamaad caliyo o u\n",
            "daafayey sideedh taahsan kama a tirak ale ah oo dhawanaayaayey intau oo hartey u\n",
            "anafalehay dertedah waxoo qoyskiisa aawadeen acafimaada iyo biyo yaraah o aad u\n",
            "leedahay markii roobyadii dhab sana iyo bur ah ee mar tay maraaq aya kadhiban\n",
            "jiriin ninkana edontah ay u kasabtey waxoo qayska kisa u dabah saldadadeen ay\n",
            "qaraboo yihiin oo kunool magaalaha ooga qaqiyo waxaa ey hada hal watiisa\n",
            "qasadadeen raashiinta waan kiir ah oo aiso ugu jirey buur barris iyo sonkor oo\n",
            "aiso aiso dabadeen acadiibishay bilha ladah iyo sanadkaa\n",
            "Line 9: sababti oo aanuu nolalah islami inuu u baahana oo uu ka gaagixaa waa ni baahiye\n",
            "haada wuu ee aa oqaf u baahdaa biyaa ku sinii oo tiiyaw waanka dhimah waa tiiyaw\n",
            "nola qaagamaan ya u qulubanayoo haadishey ku dhaxiin waley dhibaato aad haa ku\n",
            "gamnaaa ya waalo o dharkii uu na qabanay oo aa ku dhaxiin cabdhulaahi aya\n",
            "shaegay inoo wax oogowayn xoolo aiso dhmaneyaa ay tahay biyeeer diganka ka jiraa\n",
            "waxoo utilamamey in ay u tirsanaiyeen biyaaa gala barahaa o kuliya kuwasa aye\n",
            "kadamarey Marki ay u mudan istcimaalkaayi waxa u tirmami iney hada biya meeshaa\n",
            "u gudo ay u kadan karaayeen ay tay aiso biya talaaay u jirtaa 40km halki foosta\n",
            "aya aiso lagu keenay deganka ooo todabadh doolaa taas aa u odaydeen in ay gatan\n",
            "Line 10: waxo u xusaayaa in booyada loo biladoodeeno kale qaatanaayo ay hada u diideen\n",
            "Marki ay u yaqaashaan 3 taahsan ah wax oo hado ka qawiyeen dadaka iyo yaraan\n",
            "dhabe ey ifaantaan ay kuwa aiso ay hada ka dhibanyahay dumaay u saarnayey cabi\n",
            "jirraayo dharkada reebaayad ka tababaana waa walah aanu wixi aad waad duwan aad\n",
            "ogoorbaah xaqqa haasa\n",
            "\n",
            "==== Basic Statistics ====\n",
            "Total lines: 24\n",
            "Total words: 2455\n",
            "Average words per line: 102.29\n",
            "Average word length: 5.17\n",
            "Adjacent word repetitions: 10\n",
            "\n",
            "==== Most Common Words ====\n",
            "u: 119\n",
            "ay: 60\n",
            "oo: 53\n",
            "ka: 48\n",
            "iyo: 35\n",
            "ku: 35\n",
            "in: 31\n",
            "uu: 30\n",
            "ah: 25\n",
            "aiso: 25\n",
            "\n",
            "==== Commonly Repeated Segments ====\n",
            "'ay u' appears 11 times\n",
            "'u u' appears 7 times\n",
            "'aad u' appears 7 times\n",
            "'waxa u' appears 7 times\n",
            "'ay ku' appears 6 times\n",
            "'a u' appears 6 times\n",
            "'in ay' appears 5 times\n",
            "'aan u' appears 5 times\n",
            "'iyo biyo' appears 4 times\n",
            "'biyo yari' appears 4 times\n",
            "\n",
            "Approximate number of sentences: 7\n",
            "\n",
            "==== Sample Sentences ====\n",
            "Sentence 1: Halkanina waa Radiyo Ergo ee codka arimaha bini'aadanimada oo fadhigiisu yahay\n",
            "magalada Nairobi ee dalka kenya waxad naga dhageysanaysaan mowjado gaaban ee\n",
            "dherer keedu yahay 22 ka mitir baan una dhiganta 25,670\n",
            "Sentence 2: 00 maga hertz\n",
            "Sentence 3: Saacada geeska afrika dhabari marka ay tahay sedexda ilaa afarta gelabnimo\n",
            "Waxaad sidoo kale naga dhagaysanaysaan qaara ka tirsan idaacadaha dalka iyo\n",
            "barta aanu ku leenahay internetka ee fadhigeedu yahay radiyo ergo dot o w r g\n",
            "Sentence 4: Kulanti wanaag san eedageystayal waa sabti ay bisha maarso sanadku waa 2022 ku\n",
            "soo dhawaada idaca Ergo ee maanta anigu ahsan madaale ayaa la socodsiinayaa\n",
            "qodobada aadka macquul ka doontan waxa ka mid ah xooldhaqatada noolol xumagaa ku\n",
            "waajahay iyo diganka cali waal ee gobal ku nado o markii daqalaah aan iyo biyo\n",
            "yari ay u dhinten in ka badan xoolahoodi\n",
            "Sentence 5: Qoraal sokono wax magalada fasah dherr e gobal ka baydh waa ugu dilah maareynta\n",
            "dhaarasho quusaska kooda markii ay dooreen kooxda al shabaab\n",
            "\n",
            "==== Line Length Analysis ====\n",
            "Shortest line: 26 words\n",
            "Longest line: 218 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cVuKbYG6cjt"
      },
      "source": [
        "# 3. Performance Overview  \n",
        "\n",
        "# 📄 Somali Audio Transcription Model Evaluation\n",
        "\n",
        "This document outlines the performance of three different AI models used to transcribe Somali-language audio. The objective was to identify a model capable of accurately converting spoken Somali into text using the **correct Latin script**. The evaluation moved from general-purpose models to more specialized ones, culminating in the success of **Gemini 2.0 Flash**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 🧠 OpenAI Whisper (Standard Model)\n",
        "\n",
        "### 🔍 Summary:\n",
        "OpenAI's standard Whisper model was the initial choice, given its general-purpose transcription capability.\n",
        "\n",
        "- **✅ Language Detection:** Correctly identified Somali.\n",
        "- **❌ Script Used:** Incorrectly transcribed text using the **Arabic script**, not the standard **Somali Latin script**.\n",
        "\n",
        "### ⚠️ Key Issue:\n",
        "The transcription output was nonsensical and unusable. It was filled with repeated Arabic phrases like:\n",
        "\n",
        "> **\"موضوع موضوع موضوع...\"** (Arabic for \"subject\")\n",
        "\n",
        "### 📉 Transcription Quality:\n",
        "- Repetitive\n",
        "- Unintelligible\n",
        "- Wrong script\n",
        "\n",
        "### 🧾 Conclusion:\n",
        "**❌ Failure.** The model was unable to produce text in the appropriate script, making it ineffective for this task.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🧬 Whisper Small Somali (`steja/whisper-small-somali`)\n",
        "\n",
        "### 🔍 Summary:\n",
        "A specialized model from Hugging Face, fine-tuned specifically for Somali, was tested to address the script issue.\n",
        "\n",
        "- **✅ Script Used:** Correctly used **Somali Latin script**\n",
        "- **✅ Language:** Identified and transcribed Somali\n",
        "\n",
        "### ⚠️ Key Issue:\n",
        "The model produced **hallucinatory repetitions**, frequently looping on phrases like:\n",
        "\n",
        "> *\"iyo iyo iyo,\" \"dhul dhul dhul,\" \"dheesho dheesho...\"*\n",
        "\n",
        "### 📉 Transcription Quality:\n",
        "- Correct script, but\n",
        "- High frequency of repetitive nonsense\n",
        "- Difficult to interpret meaningful content\n",
        "\n",
        "### 🧾 Conclusion:\n",
        "**⚠️ Partial Success.** Script issue resolved, but the hallucinations rendered the transcriptions unreliable for practical use.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🚀 Gemini 2.0 Flash (Google)\n",
        "\n",
        "### 🔍 Summary:\n",
        "The final model tested, **Gemini 2.0 Flash**, exhibited strong performance and significantly improved output quality.\n",
        "\n",
        "- **✅ Script:** Somali Latin script\n",
        "- **✅ Structure:** Coherent sentences and paragraphs\n",
        "- **✅ Repetition:** Natural and minimal\n",
        "- **✅ Accuracy:** High contextual relevance\n",
        "\n",
        "### ✅ Transcription Quality:\n",
        "- Well-structured\n",
        "- Accurate\n",
        "- Readable and ready for analysis\n",
        "\n",
        "### 🧾 Conclusion:\n",
        "**✅ Success.** Gemini 2.0 Flash delivered the best results. It provided **accurate**, **natural**, and **usable** transcriptions aligned with the project’s goals.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}